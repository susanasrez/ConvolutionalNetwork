{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t_o52j1ZdFr0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.utils as vutils\n",
        "from torchvision import models, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from facenet_pytorch import InceptionResnetV1, MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P243vsC6eXt6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando cpu para entrenar\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Usando {device} para entrenar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cE24-k9eaIO"
      },
      "source": [
        "## Cargar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vXx0yLhMebN3"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "DATASET = os.path.abspath('real_and_fake_face/')\n",
        "INITIAL_SIZE_OF_IMAGES = (160, 160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OqD13cs4edsr"
      },
      "outputs": [],
      "source": [
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(INITIAL_SIZE_OF_IMAGES),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalizar cada canal\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=DATASET, transform=data_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wHkzTBMGeeVU"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "# Transformaciones a los conjuntos de datos (train, valid, test)\n",
        "train_dataset.transform = data_transforms\n",
        "valid_dataset.transform = data_transforms\n",
        "test_dataset.transform = data_transforms\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porcentaje de cada clase en train:\n",
            "Porcentaje de 0: 47.24264705882353%\n",
            "Porcentaje de 1: 52.75735294117647%\n"
          ]
        }
      ],
      "source": [
        "print('Porcentaje de cada clase en train:')\n",
        "\n",
        "total_0 = 0\n",
        "total_1 = 0\n",
        "for images, labels in train_loader:\n",
        "    total_0 += (labels == 0).sum().item()\n",
        "    total_1 += (labels == 1).sum().item()\n",
        "\n",
        "total_samples = len(train_loader.dataset)\n",
        "porcentaje_0 = (total_0 / total_samples) * 100\n",
        "porcentaje_1 = (total_1 / total_samples) * 100\n",
        "\n",
        "print(f'Porcentaje de 0: {porcentaje_0}%')\n",
        "print(f'Porcentaje de 1: {porcentaje_1}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlwxX7ntegpC"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NLaO1PK6egBa"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64*16*16, 128)\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        \n",
        "        self.fc3 = nn.Linear(64, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.leaky_relu(self.conv1(x),negative_slope = 0.01))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        x = x.view(-1, 64*16*16)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=16384, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = CNN().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\marap\\OneDrive\\Documentos\\ProyectosVisual\\ConvolutionalNetwork3\\ConvolutionalNetwork\\Proyecto.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m correct_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m total_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torchvision\\datasets\\folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(sample)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39;49mdiv(\u001b[39m255\u001b[39;49m)\n\u001b[0;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNBatch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNBatch, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(32*32*32, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "\n",
        "        x = x.view(-1, 32*32*32)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3wFmmXY6ejPK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNBatch(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = CNNBatch().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 2.3520, Train Acc: 50.74%, Valid Loss: 0.6891, Valid Acc: 54.90%\n",
            "Epoch [2/10], Train Loss: 0.7906, Train Acc: 50.06%, Valid Loss: 0.6870, Valid Acc: 54.90%\n",
            "Epoch [3/10], Train Loss: 0.7116, Train Acc: 50.31%, Valid Loss: 0.6942, Valid Acc: 45.59%\n",
            "Epoch [4/10], Train Loss: 0.7058, Train Acc: 50.74%, Valid Loss: 0.6925, Valid Acc: 51.96%\n",
            "Epoch [5/10], Train Loss: 0.6969, Train Acc: 50.80%, Valid Loss: 0.6928, Valid Acc: 52.45%\n",
            "Epoch [6/10], Train Loss: 0.6951, Train Acc: 52.88%, Valid Loss: 0.6900, Valid Acc: 50.98%\n",
            "Epoch [7/10], Train Loss: 0.6952, Train Acc: 52.08%, Valid Loss: 0.6857, Valid Acc: 54.90%\n",
            "Epoch [8/10], Train Loss: 0.6947, Train Acc: 53.62%, Valid Loss: 0.6922, Valid Acc: 45.59%\n",
            "Epoch [9/10], Train Loss: 0.6897, Train Acc: 54.23%, Valid Loss: 0.6857, Valid Acc: 55.39%\n",
            "Epoch [10/10], Train Loss: 0.6900, Train Acc: 53.80%, Valid Loss: 0.6813, Valid Acc: 50.98%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 3. LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNLeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNLeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5,padding=1) \n",
        "        \n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5,padding=1)\n",
        "        self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(120*28*28, 84)\n",
        "        self.fc2 = nn.Linear(84, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.tanh(self.conv1(x)))\n",
        "        x = self.pool(F.tanh(self.conv2(x)))\n",
        "        x = F.tanh(self.conv3(x))\n",
        "\n",
        "        x = x.view(-1, 120*28*28)\n",
        "\n",
        "        x = self.dropout(F.tanh(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNLeNet(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (fc1): Linear(in_features=94080, out_features=84, bias=True)\n",
            "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = CNNLeNet().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.8383, Train Acc: 51.47%, Valid Loss: 0.6942, Valid Acc: 54.41%\n",
            "Epoch [2/10], Train Loss: 0.7611, Train Acc: 49.94%, Valid Loss: 0.6904, Valid Acc: 53.92%\n",
            "Epoch [3/10], Train Loss: 0.7489, Train Acc: 50.25%, Valid Loss: 0.6919, Valid Acc: 54.41%\n",
            "Epoch [4/10], Train Loss: 0.7361, Train Acc: 50.86%, Valid Loss: 0.6875, Valid Acc: 54.90%\n",
            "Epoch [5/10], Train Loss: 0.7362, Train Acc: 50.00%, Valid Loss: 0.6904, Valid Acc: 54.41%\n",
            "Epoch [6/10], Train Loss: 0.7320, Train Acc: 47.37%, Valid Loss: 0.6894, Valid Acc: 53.92%\n",
            "Epoch [7/10], Train Loss: 0.7070, Train Acc: 52.08%, Valid Loss: 0.6896, Valid Acc: 54.41%\n",
            "Epoch [8/10], Train Loss: 0.7061, Train Acc: 51.65%, Valid Loss: 0.6877, Valid Acc: 54.41%\n",
            "Epoch [9/10], Train Loss: 0.7112, Train Acc: 49.26%, Valid Loss: 0.6882, Valid Acc: 54.41%\n",
            "Epoch [10/10], Train Loss: 0.7129, Train Acc: 49.26%, Valid Loss: 0.6916, Valid Acc: 53.92%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNLeNetBatch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNLeNetBatch, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(6) \n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5,padding=1) \n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5,padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(120)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(120*28*28, 84)  # Capa F6\n",
        "        self.fc2 = nn.Linear(84, 64)\n",
        "        self.fc3 = nn.Linear(64, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(-1, 120*28*28)\n",
        "\n",
        "        x = self.dropout(F.tanh(self.fc1(x)))\n",
        "        x = self.dropout(F.tanh(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNLeNetBatch(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (fc1): Linear(in_features=94080, out_features=84, bias=True)\n",
            "  (fc2): Linear(in_features=84, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = CNNLeNetBatch().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.7634, Train Acc: 48.84%, Valid Loss: 0.6867, Valid Acc: 54.90%\n",
            "Epoch [2/10], Train Loss: 0.7154, Train Acc: 50.37%, Valid Loss: 0.6864, Valid Acc: 54.90%\n",
            "Epoch [3/10], Train Loss: 0.7173, Train Acc: 49.39%, Valid Loss: 0.6923, Valid Acc: 54.90%\n",
            "Epoch [4/10], Train Loss: 0.7048, Train Acc: 49.82%, Valid Loss: 0.6879, Valid Acc: 54.90%\n",
            "Epoch [5/10], Train Loss: 0.7064, Train Acc: 51.23%, Valid Loss: 0.6895, Valid Acc: 54.90%\n",
            "Epoch [6/10], Train Loss: 0.7105, Train Acc: 50.43%, Valid Loss: 0.6911, Valid Acc: 54.90%\n",
            "Epoch [7/10], Train Loss: 0.7028, Train Acc: 50.74%, Valid Loss: 0.6896, Valid Acc: 54.90%\n",
            "Epoch [8/10], Train Loss: 0.7016, Train Acc: 50.18%, Valid Loss: 0.6910, Valid Acc: 54.90%\n",
            "Epoch [9/10], Train Loss: 0.7007, Train Acc: 49.88%, Valid Loss: 0.6871, Valid Acc: 54.90%\n",
            "Epoch [10/10], Train Loss: 0.6959, Train Acc: 50.80%, Valid Loss: 0.6874, Valid Acc: 54.90%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNLeNetBatch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNLeNetBatch, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32) \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,padding=1) \n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3,padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64*16*16, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(-1, 64*16*16)\n",
        "\n",
        "        x = self.dropout(F.tanh(self.fc1(x)))\n",
        "        x = self.dropout(F.tanh(self.fc2(x)))\n",
        "        x = self.dropout(F.tanh(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNLeNetBatch(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (fc1): Linear(in_features=16384, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc4): Linear(in_features=32, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = CNNLeNetBatch().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KPouW2Bxel3F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.7016, Train Acc: 52.94%, Valid Loss: 0.6902, Valid Acc: 52.45%\n",
            "Epoch [2/10], Train Loss: 0.7133, Train Acc: 49.69%, Valid Loss: 0.6895, Valid Acc: 54.90%\n",
            "Epoch [3/10], Train Loss: 0.6994, Train Acc: 52.88%, Valid Loss: 0.6949, Valid Acc: 46.57%\n",
            "Epoch [4/10], Train Loss: 0.7016, Train Acc: 50.25%, Valid Loss: 0.6862, Valid Acc: 54.90%\n",
            "Epoch [5/10], Train Loss: 0.7020, Train Acc: 50.98%, Valid Loss: 0.6960, Valid Acc: 45.59%\n",
            "Epoch [6/10], Train Loss: 0.7007, Train Acc: 51.41%, Valid Loss: 0.6866, Valid Acc: 54.90%\n",
            "Epoch [7/10], Train Loss: 0.6985, Train Acc: 52.45%, Valid Loss: 0.6901, Valid Acc: 54.90%\n",
            "Epoch [8/10], Train Loss: 0.6937, Train Acc: 51.35%, Valid Loss: 0.6859, Valid Acc: 54.90%\n",
            "Epoch [9/10], Train Loss: 0.6966, Train Acc: 51.23%, Valid Loss: 0.6895, Valid Acc: 54.90%\n",
            "Epoch [10/10], Train Loss: 0.6959, Train Acc: 50.55%, Valid Loss: 0.6907, Valid Acc: 49.51%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 2 avg\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.1, rho=0.95, eps=1e-8, weight_decay=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNBatch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNBatch, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(32*32*32, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "\n",
        "        x = x.view(-1, 32*32*32)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNBatch(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = CNNBatch().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=0.1)\n",
        "])\n",
        "\n",
        "train_loader.transform = transform\n",
        "valid_loader.transform = transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.6940, Train Acc: 53.98%, Valid Loss: 0.6864, Valid Acc: 54.90%\n",
            "Epoch [2/10], Train Loss: 0.6812, Train Acc: 55.51%, Valid Loss: 0.6750, Valid Acc: 54.90%\n",
            "Epoch [3/10], Train Loss: 0.6632, Train Acc: 59.50%, Valid Loss: 0.6775, Valid Acc: 54.90%\n",
            "Epoch [4/10], Train Loss: 0.6623, Train Acc: 59.99%, Valid Loss: 0.6665, Valid Acc: 59.31%\n",
            "Epoch [5/10], Train Loss: 0.6553, Train Acc: 61.09%, Valid Loss: 0.6596, Valid Acc: 59.31%\n",
            "Epoch [6/10], Train Loss: 0.6384, Train Acc: 63.91%, Valid Loss: 0.7033, Valid Acc: 57.35%\n",
            "Epoch [7/10], Train Loss: 0.6380, Train Acc: 64.58%, Valid Loss: 0.6530, Valid Acc: 59.31%\n",
            "Epoch [8/10], Train Loss: 0.6194, Train Acc: 66.85%, Valid Loss: 0.6667, Valid Acc: 54.41%\n",
            "Epoch [9/10], Train Loss: 0.6144, Train Acc: 66.79%, Valid Loss: 0.6404, Valid Acc: 56.86%\n",
            "Epoch [10/10], Train Loss: 0.6038, Train Acc: 66.61%, Valid Loss: 0.6559, Valid Acc: 60.29%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "50o0oEowenIi"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(valid_losses, label='Valid Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Loss vs. Epochs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(valid_accuracies, label='Valid Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Accuracy vs. Epochs')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "S_7oru0qenlJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisión del modelo en las 10000 imágenes de prueba: 65.36585365853658%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad(): # Deshabilita el cálculo de gradientes\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        # pasar al dispositivo\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        ############################\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1) # Obtiene el índice de la clase con mayor probabilidad\n",
        "        total += labels.size(0) # Tamaño del batch\n",
        "        correct += (predicted == labels).sum().item() # Suma el número de predicciones correctas\n",
        "\n",
        "    print(f'Precisión del modelo en las 10000 imágenes de prueba: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probarndo el transferLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import models, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "alexnet = models.alexnet(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in alexnet.features.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "alexnet.classifier = nn.Sequential(\n",
        "    nn.Linear(256 * 6 * 6, 1024), \n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1024, 512),  # Menos neuronas en esta capa\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(256, num_classes)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 1.0224, Train Acc: 50.12%, Valid Loss: 0.6835, Valid Acc: 54.90%\n",
            "Epoch [2/10], Train Loss: 0.7167, Train Acc: 53.74%, Valid Loss: 0.6850, Valid Acc: 55.39%\n",
            "Epoch [3/10], Train Loss: 0.6919, Train Acc: 53.74%, Valid Loss: 0.6708, Valid Acc: 55.39%\n",
            "Epoch [4/10], Train Loss: 0.6785, Train Acc: 56.13%, Valid Loss: 0.6615, Valid Acc: 57.84%\n",
            "Epoch [5/10], Train Loss: 0.6486, Train Acc: 62.62%, Valid Loss: 0.6675, Valid Acc: 57.84%\n",
            "Epoch [6/10], Train Loss: 0.6407, Train Acc: 62.07%, Valid Loss: 0.6384, Valid Acc: 54.90%\n",
            "Epoch [7/10], Train Loss: 0.6226, Train Acc: 63.97%, Valid Loss: 0.6382, Valid Acc: 56.86%\n",
            "Epoch [8/10], Train Loss: 0.6073, Train Acc: 67.28%, Valid Loss: 0.6855, Valid Acc: 62.25%\n",
            "Epoch [9/10], Train Loss: 0.5618, Train Acc: 69.98%, Valid Loss: 0.6519, Valid Acc: 54.90%\n",
            "Epoch [10/10], Train Loss: 0.5517, Train Acc: 71.75%, Valid Loss: 0.6852, Valid Acc: 61.27%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    alexnet.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = alexnet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "    alexnet.train()\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ResNet 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando cuda para entrenar\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Usando {device} para entrenar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "\n",
        "massive_data_augmentation = transforms.Compose([\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización\n",
        "])\n",
        "\n",
        "train_dataset.dataset.transform = massive_data_augmentation\n",
        "valid_dataset.dataset.transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "test_dataset.dataset.transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Crear DataLoader para cada conjunto de datos (train, valid, test)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "resnet18 = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in resnet18.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Sequential' object has no attribute 'in_features'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\marap\\OneDrive\\Documentos\\ProyectosVisual\\ConvolutionalNetwork3\\ConvolutionalNetwork\\Proyecto.ipynb Cell 46\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, 2)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m resnet18\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(resnet18\u001b[39m.\u001b[39;49mfc\u001b[39m.\u001b[39;49min_features, \u001b[39m512\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     nn\u001b[39m.\u001b[39mReLU(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.5\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m512\u001b[39m, \u001b[39m256\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     nn\u001b[39m.\u001b[39mReLU(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.3\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m256\u001b[39m, \u001b[39m2\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#X62sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'in_features'"
          ]
        }
      ],
      "source": [
        "#resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, 2)\n",
        "resnet18.fc = nn.Sequential(\n",
        "    nn.Linear(resnet18.fc.in_features, 512),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 2),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 1.6099, Train Acc: 48.53%, Valid Loss: 1.4629, Valid Acc: 52.45%\n",
            "Epoch [2/10], Train Loss: 0.9037, Train Acc: 50.37%, Valid Loss: 0.8107, Valid Acc: 47.06%\n",
            "Epoch [3/10], Train Loss: 0.8279, Train Acc: 51.29%, Valid Loss: 0.7817, Valid Acc: 53.92%\n",
            "Epoch [4/10], Train Loss: 0.8120, Train Acc: 51.96%, Valid Loss: 0.7698, Valid Acc: 53.92%\n",
            "Epoch [5/10], Train Loss: 0.7573, Train Acc: 55.76%, Valid Loss: 0.8116, Valid Acc: 52.45%\n",
            "Epoch [6/10], Train Loss: 0.7512, Train Acc: 55.64%, Valid Loss: 1.0849, Valid Acc: 47.55%\n",
            "Epoch [7/10], Train Loss: 0.7735, Train Acc: 57.29%, Valid Loss: 0.7873, Valid Acc: 52.94%\n",
            "Epoch [8/10], Train Loss: 0.7232, Train Acc: 57.66%, Valid Loss: 0.7361, Valid Acc: 54.41%\n",
            "Epoch [9/10], Train Loss: 0.7524, Train Acc: 57.48%, Valid Loss: 0.7945, Valid Acc: 55.88%\n",
            "Epoch [10/10], Train Loss: 0.6582, Train Acc: 64.15%, Valid Loss: 0.7373, Valid Acc: 58.33%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "resnet18 = models.resnet18().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adadelta(resnet18.parameters(), lr=1)\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    resnet18.train()\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = resnet18(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    resnet18.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = resnet18(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet 34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Susana\\anaconda3\\envs\\aa2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Susana\\anaconda3\\envs\\aa2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=128, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet34(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.fc = torch.nn.Sequential(\n",
        "    torch.nn.Linear(model.fc.in_features, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5),\n",
        "    torch.nn.Linear(256, 128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5),\n",
        "    torch.nn.Linear(128, 2)\n",
        ")\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.6962, Train Acc: 52.63%, Valid Loss: 0.7040, Valid Acc: 50.00%\n",
            "Epoch [2/10], Train Loss: 0.6947, Train Acc: 53.43%, Valid Loss: 0.6891, Valid Acc: 55.88%\n",
            "Epoch [3/10], Train Loss: 0.6849, Train Acc: 54.72%, Valid Loss: 0.6842, Valid Acc: 50.98%\n",
            "Epoch [4/10], Train Loss: 0.6826, Train Acc: 57.41%, Valid Loss: 0.6774, Valid Acc: 58.82%\n",
            "Epoch [5/10], Train Loss: 0.6736, Train Acc: 57.97%, Valid Loss: 0.6868, Valid Acc: 62.75%\n",
            "Epoch [6/10], Train Loss: 0.6707, Train Acc: 58.52%, Valid Loss: 0.6705, Valid Acc: 56.37%\n",
            "Epoch [7/10], Train Loss: 0.6651, Train Acc: 59.01%, Valid Loss: 0.6713, Valid Acc: 58.82%\n",
            "Epoch [8/10], Train Loss: 0.6540, Train Acc: 60.60%, Valid Loss: 0.6759, Valid Acc: 59.80%\n",
            "Epoch [9/10], Train Loss: 0.6554, Train Acc: 60.91%, Valid Loss: 0.6496, Valid Acc: 56.37%\n",
            "Epoch [10/10], Train Loss: 0.6381, Train Acc: 63.36%, Valid Loss: 0.7243, Valid Acc: 55.88%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "#OJo modelo\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adadelta(model.fc.parameters(), lr=0.1)\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FaceNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = InceptionResnetV1(pretrained='vggface2', classify=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = model.last_linear.in_features\n",
        "\n",
        "#Cambiar esta parte\n",
        "custom_fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 2), \n",
        ")\n",
        "\n",
        "model.last_linear = custom_fc\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "running_mean should contain 2 elements not 512",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\marap\\OneDrive\\Documentos\\ProyectosVisual\\ConvolutionalNetwork3\\ConvolutionalNetwork\\Proyecto.ipynb Cell 55\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#Y113sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#Y113sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#Y113sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#Y113sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marap/OneDrive/Documentos/ProyectosVisual/ConvolutionalNetwork3/ConvolutionalNetwork/Proyecto.ipynb#Y113sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:297\u001b[0m, in \u001b[0;36mInceptionResnetV1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    295\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[0;32m    296\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_linear(x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m--> 297\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_bn(x)\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassify:\n\u001b[0;32m    299\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits(x)\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\marap\\miniconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2479\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2480\u001b[0m )\n",
            "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 2 elements not 512"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "train_losses = []   \n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            model.eval()\n",
        "            torch.set_grad_enabled(False)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MTCNN(keep_all=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VGGFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "\n",
        "massive_data_augmentation = transforms.Compose([\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización\n",
        "])\n",
        "\n",
        "train_dataset.dataset.transform = massive_data_augmentation\n",
        "valid_dataset.dataset.transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "test_dataset.dataset.transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "InceptionResnetV1(\n",
            "  (conv2d_1a): BasicConv2d(\n",
            "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (conv2d_2a): BasicConv2d(\n",
            "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (conv2d_2b): BasicConv2d(\n",
            "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (maxpool_3a): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2d_3b): BasicConv2d(\n",
            "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (conv2d_4a): BasicConv2d(\n",
            "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (conv2d_4b): BasicConv2d(\n",
            "    (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (repeat_1): Sequential(\n",
            "    (0): Block35(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (branch2): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (1): Block35(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (branch2): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): Block35(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (branch2): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (3): Block35(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (branch2): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (4): Block35(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (branch2): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (mixed_6a): Mixed_6a(\n",
            "    (branch0): BasicConv2d(\n",
            "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (branch1): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (2): BasicConv2d(\n",
            "        (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (branch2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (repeat_2): Sequential(\n",
            "    (0): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (1): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (3): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (4): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (5): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (6): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (7): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (8): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (9): Block17(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (mixed_7a): Mixed_7a(\n",
            "    (branch0): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (branch1): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (2): BasicConv2d(\n",
            "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (branch3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (repeat_3): Sequential(\n",
            "    (0): Block8(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (1): Block8(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): Block8(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (3): Block8(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (4): Block8(\n",
            "      (branch0): BasicConv2d(\n",
            "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (branch1): Sequential(\n",
            "        (0): BasicConv2d(\n",
            "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (1): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "        (2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (block8): Block8(\n",
            "    (branch0): BasicConv2d(\n",
            "      (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (branch1): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (2): BasicConv2d(\n",
            "        (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (avgpool_1a): AdaptiveAvgPool2d(output_size=1)\n",
            "  (dropout): Dropout(p=0.6, inplace=False)\n",
            "  (last_linear): Linear(in_features=1792, out_features=512, bias=False)\n",
            "  (last_bn): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (logits): Linear(in_features=512, out_features=8631, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "extractor = InceptionResnetV1(pretrained='vggface2')\n",
        "print(extractor)\n",
        "\n",
        "for param in extractor.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Clasificador(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Clasificador, self).__init__()\n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clasificador(\n",
            "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Clasificador().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.1)\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 1.9314, Train Acc: 50.61%, Valid Loss: 0.7067, Valid Acc: 53.92%\n",
            "Epoch [2/10], Train Loss: 0.7749, Train Acc: 50.25%, Valid Loss: 0.6925, Valid Acc: 54.41%\n",
            "Epoch [3/10], Train Loss: 0.7377, Train Acc: 50.49%, Valid Loss: 0.6919, Valid Acc: 54.41%\n",
            "Epoch [4/10], Train Loss: 0.7469, Train Acc: 52.14%, Valid Loss: 0.6946, Valid Acc: 45.59%\n",
            "Epoch [5/10], Train Loss: 0.6958, Train Acc: 49.94%, Valid Loss: 0.6886, Valid Acc: 54.41%\n",
            "Epoch [6/10], Train Loss: 0.7009, Train Acc: 48.77%, Valid Loss: 0.6949, Valid Acc: 45.59%\n",
            "Epoch [7/10], Train Loss: 0.6949, Train Acc: 51.41%, Valid Loss: 0.6877, Valid Acc: 54.41%\n",
            "Epoch [8/10], Train Loss: 0.6935, Train Acc: 51.84%, Valid Loss: 0.6878, Valid Acc: 54.41%\n",
            "Epoch [9/10], Train Loss: 0.6936, Train Acc: 52.27%, Valid Loss: 0.6929, Valid Acc: 54.41%\n",
            "Epoch [10/10], Train Loss: 0.6948, Train Acc: 50.43%, Valid Loss: 0.6979, Valid Acc: 45.59%\n",
            "Entrenamiento finalizado!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        caracteristicas = extractor(images)\n",
        "        outputs = model(caracteristicas)\n",
        "\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            caracteristicas = extractor(images)\n",
        "            outputs = model(caracteristicas)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probando otro clasificador "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Clasificador(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Clasificador, self).__init__()\n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 2)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        nn.init.kaiming_normal_(self.fc3.weight)\n",
        "\n",
        "        self.fc1.weight_regularizer = nn.Parameter(torch.Tensor(1), requires_grad=True)\n",
        "        self.fc2.weight_regularizer = nn.Parameter(torch.Tensor(1), requires_grad=True)\n",
        "        self.fc3.weight_regularizer = nn.Parameter(torch.Tensor(1), requires_grad=True)\n",
        "        self.regularization_strength = 0.001\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        regularization_loss = (\n",
        "            self.regularization_strength * (\n",
        "                torch.norm(self.fc1.weight) +\n",
        "                torch.norm(self.fc2.weight) +\n",
        "                torch.norm(self.fc3.weight)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return x, regularization_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clasificador(\n",
            "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Clasificador().to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Susana\\Desktop\\Universidad\\Tercero\\Primer\\AA2\\project\\ConvolutionalNetwork\\Proyecto.ipynb Cell 70\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Susana/Desktop/Universidad/Tercero/Primer/AA2/project/ConvolutionalNetwork/Proyecto.ipynb#Y140sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m caracteristicas \u001b[39m=\u001b[39m extractor(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Susana/Desktop/Universidad/Tercero/Primer/AA2/project/ConvolutionalNetwork/Proyecto.ipynb#Y140sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(caracteristicas)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Susana/Desktop/Universidad/Tercero/Primer/AA2/project/ConvolutionalNetwork/Proyecto.ipynb#Y140sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Susana/Desktop/Universidad/Tercero/Primer/AA2/project/ConvolutionalNetwork/Proyecto.ipynb#Y140sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Susana/Desktop/Universidad/Tercero/Primer/AA2/project/ConvolutionalNetwork/Proyecto.ipynb#Y140sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
            "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\aa2\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
            "\u001b[1;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    training_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        caracteristicas = extractor(images)\n",
        "        outputs = model(caracteristicas)\n",
        "\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        training_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = training_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            caracteristicas = extractor(images)\n",
        "            outputs = model(caracteristicas)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
        "\n",
        "print(\"Entrenamiento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ## Casia-webface\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = InceptionResnetV1(pretrained='casia-webface').eval()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
